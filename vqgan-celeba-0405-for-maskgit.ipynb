{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.097717Z",
     "iopub.status.busy": "2024-04-12T06:37:15.097341Z",
     "iopub.status.idle": "2024-04-12T06:37:15.103719Z",
     "shell.execute_reply": "2024-04-12T06:37:15.102631Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.097689Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.106448Z",
     "iopub.status.busy": "2024-04-12T06:37:15.105507Z",
     "iopub.status.idle": "2024-04-12T06:37:15.112681Z",
     "shell.execute_reply": "2024-04-12T06:37:15.111769Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.106405Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#from torchvision.transforms import v2 ####\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "try:\n",
    "    import lpips\n",
    "except:\n",
    "    pass\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.114534Z",
     "iopub.status.busy": "2024-04-12T06:37:15.114266Z",
     "iopub.status.idle": "2024-04-12T06:37:15.131350Z",
     "shell.execute_reply": "2024-04-12T06:37:15.130454Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.114512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Availability: True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(12)\n",
    "\n",
    "phase = 2\n",
    "# 1: training vae/gan & get encoding dataset\n",
    "# 2: training transformer\n",
    "# 3: product phase\n",
    "\n",
    "config = {'ver': f'afhq_maskgit_0412_v02_phase{phase}',\n",
    "          'description': 'train_again',\n",
    "          'activation': 'relu', 'num_res_blocks': 6, 'conv_in_channels': 64,\n",
    "          'res_bottle_neck_factor': 2,\n",
    "          'channels_mult': (1,2,2,3,4),\n",
    "          'embed_dim': 128, 'n_embed': 512,\n",
    "          'codebook_beta': 0.25,\n",
    "          'base_learning_rate': 3.0e-4, \n",
    "          'min_learning_rate':1e-4,\n",
    "          'gan_loss_weight': 0.03,\n",
    "          'batch_size': 32,\n",
    "          'max_epochs': 40, 'train_test_split': 0.9,\n",
    "          'save_every_n_epoch':40,\n",
    "          'dataset_size': 15228,\n",
    "          'rec_loss_weight': 1 ,#/ 0.06327039811675479 / 4, # pixel level rec loss\n",
    "          'lpips_type': 'vgg',\n",
    "          'lpips_loss_weight': 1,\n",
    "          'cuda': torch.cuda.is_available(),\n",
    "          'image_size': (256,256),###\n",
    "          'load': '/kaggle/input/afhq-vqvae-0412-v01/vq12.pth',\n",
    "          'save': True,\n",
    "          'use_disc': False,\n",
    "          'phase': phase  \n",
    "          } \n",
    "#if phase!=1:\n",
    "#    assert (not config['use_disc']) and config['lpips_loss_weight']<=0\n",
    "\n",
    "config['Ds_ratio'] = 2**(len(config['channels_mult'])-1)\n",
    "config['latent_size']=(round(config['image_size'][0]/config['Ds_ratio']),\n",
    "                       round(config['image_size'][1]/config['Ds_ratio']))\n",
    "config['one_gan_loss_for_x_rec_loss'] = config['gan_loss_weight']/config['rec_loss_weight']\n",
    "\n",
    "\n",
    "d_config = {'channels_mult': (1,2,4),\n",
    "            'conv_in_channels': 64,\n",
    "            'base_learning_rate': 8e-5\n",
    "           }\n",
    "\n",
    "d_config['Ds_ratio'] = 2**(len(d_config['channels_mult']))\n",
    "d_config['patch_size']=(round(config['image_size'][0]/d_config['Ds_ratio']),\n",
    "                        round(config['image_size'][1]/d_config['Ds_ratio']))\n",
    "\n",
    "t_config = {'n_pos':256,\n",
    "            'n_tokens':512,\n",
    "            'embed_dim':192,\n",
    "            'nhead':8,\n",
    "            'hidden_dim':768,\n",
    "            'n_layers':8,\n",
    "            'n_steps':10,\n",
    "            'base_learning_rate':2e-4,\n",
    "            'min_learning_rate':5e-5,\n",
    "            'batch_size':16,\n",
    "            'batch_acc':1,\n",
    "            'load':'/kaggle/input/maskgit-0412-v01/maskgit40.pth',\n",
    "            #'temperature':(3,1),\n",
    "           }\n",
    "\n",
    "assert t_config['n_pos']==config['latent_size'][0]*config['latent_size'][1]\n",
    "\n",
    "data_path_afhq = '/kaggle/input/afhq-512'\n",
    "data_path_celeba = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n",
    "data_path_enc = '/kaggle/input/afhq-vqvae-0412-v01/12_enc.npy'\n",
    "outcome_root = \"/kaggle/working/vqvae\"\n",
    "if not os.path.exists(os.path.join(outcome_root, config['ver'])):\n",
    "    os.makedirs(os.path.join(outcome_root, config['ver']))\n",
    "config_path = os.path.join(outcome_root, f\"{config['ver']}/config.txt\")\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(str(config) + '\\n')\n",
    "    if config['use_disc']:\n",
    "        f.write(str(d_config) + '\\n')\n",
    "    f.write(str(t_config) + '\\n')\n",
    "    #f.write(f\"N_GD: {config['max_epochs']*config['dataset_size']*config['train_test_split']/config['batch_size']:.0f}\\n\")\n",
    "\n",
    "print(f\"Cuda Availability: {config['cuda']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.248405Z",
     "iopub.status.busy": "2024-04-12T06:37:15.247693Z",
     "iopub.status.idle": "2024-04-12T06:37:15.362863Z",
     "shell.execute_reply": "2024-04-12T06:37:15.361944Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.248374Z"
    }
   },
   "outputs": [],
   "source": [
    "if config['activation'] == 'swish':\n",
    "    def activation(x):\n",
    "        return x * F.sigmoid(x)\n",
    "elif config['activation'] == 'hardswish':\n",
    "    activation = F.hardswish_\n",
    "else:\n",
    "    activation = F.relu_\n",
    "\n",
    "\n",
    "def cf2cl(tensor):\n",
    "    return torch.permute(tensor, [0, 2, 3, 1])\n",
    "\n",
    "\n",
    "def cl2cf(tensor):\n",
    "    return torch.permute(tensor, [0, 3, 1, 2])\n",
    "\n",
    "\n",
    "def norm(in_channels):\n",
    "    #return nn.Identity()\n",
    "    # return nn.BatchNorm2d(num_features=in_channels)\n",
    "    #return nn.InstanceNorm2d(num_features=in_channels, eps=1e-05, momentum=0.1, affine=False)\n",
    "    return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=False)\n",
    "\n",
    "def d_norm(in_channels):\n",
    "    return nn.Identity()\n",
    "    #return nn.BatchNorm2d(num_features=in_channels)\n",
    "    #return nn.InstanceNorm2d(num_features=in_channels, eps=1e-05, momentum=0.1, affine=False)\n",
    "    #return nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=False)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, kernel_size=3):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels if out_channels else in_channels\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                stride=2,\n",
    "                                padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, kernel_size=3):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels if out_channels else in_channels\n",
    "        \n",
    "        if kernel_size == 4:\n",
    "            self.upsample = nn.Identity()\n",
    "            self.conv = nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                           out_channels=out_channels,\n",
    "                                           kernel_size=4,\n",
    "                                           stride=2,\n",
    "                                           padding=1,\n",
    "                                           output_padding=0)\n",
    "            \n",
    "        elif kernel_size == 3:\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "            self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding='same')\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = self.upsample(x)\n",
    "        h = self.conv(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, bottle_neck_channels=None, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels if out_channels else in_channels\n",
    "        if bottle_neck_channels is not None:\n",
    "            self.bottle_neck_channels = bottle_neck_channels\n",
    "        else:\n",
    "            self.bottle_neck_channels = max(self.out_channels,\n",
    "                                            self.in_channels)\\\n",
    "                                        // config['res_bottle_neck_factor']\n",
    "            self.bottle_neck_channels = max(32, self.bottle_neck_channels)\n",
    "\n",
    "        self.norm1 = norm(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=self.bottle_neck_channels,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding='same')\n",
    "        self.norm2 = norm(self.bottle_neck_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.bottle_neck_channels,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding='same')\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.conv_shortcut = nn.Conv2d(in_channels=in_channels,\n",
    "                                           out_channels=self.out_channels,\n",
    "                                           kernel_size=1,\n",
    "                                           stride=1,\n",
    "                                           padding='same')\n",
    "        else:\n",
    "            self.conv_shortcut = nn.Identity()\n",
    "        self.rescale = 1  # / config['num_res_blocks']\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = activation(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.norm2(h)\n",
    "        h = activation(h)\n",
    "        h = self.conv2(h)\n",
    "        x = self.conv_shortcut(x)\n",
    "\n",
    "        return x + h # self.rescale\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels,embed_channels=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        if embed_channels is not None:\n",
    "            self.embed_channels = embed_channels\n",
    "        else:\n",
    "            self.embed_channels = in_channels\n",
    "\n",
    "        self.norm = norm(in_channels)\n",
    "        self.q = nn.Conv2d(in_channels,\n",
    "                           self.embed_channels,\n",
    "                           kernel_size=1,\n",
    "                           stride=1,\n",
    "                           padding=0)\n",
    "        self.k = nn.Conv2d(in_channels,\n",
    "                           self.embed_channels,\n",
    "                           kernel_size=1,\n",
    "                           stride=1,\n",
    "                           padding=0)\n",
    "        self.v = nn.Conv2d(in_channels,\n",
    "                           self.embed_channels,\n",
    "                           kernel_size=1,\n",
    "                           stride=1,\n",
    "                           padding=0)\n",
    "        self.proj_out = nn.Conv2d(self.embed_channels,\n",
    "                                  in_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  stride=1,\n",
    "                                  padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = q.reshape(b,c,h*w)\n",
    "        q = q.permute(0,2,1)   # b,hw,c\n",
    "        k = k.reshape(b,c,h*w) # b,c,hw\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,h*w)\n",
    "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3,\n",
    "                 conv_in_channels=config['conv_in_channels'],\n",
    "                 out_channels=config['embed_dim'],\n",
    "                 channels_mult=config['channels_mult']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=conv_in_channels * channels_mult[0],\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding='same')\n",
    "        current_channels = conv_in_channels * channels_mult[0]\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        for i, m in enumerate(channels_mult): \n",
    "            blk_in = current_channels\n",
    "            blk_out = conv_in_channels * m\n",
    "            if i != len(channels_mult)-1:\n",
    "                layers.append(ResBlock(in_channels=blk_in,\n",
    "                                       out_channels=blk_in))  ###\n",
    "                layers.append(DownSample(in_channels=blk_in,\n",
    "                                         out_channels=blk_out))\n",
    "                \n",
    "            else:\n",
    "                layers.append(ResBlock(in_channels=blk_in,\n",
    "                                       out_channels=blk_out))\n",
    "            current_channels = blk_out\n",
    "        self.layers=layers\n",
    "        \n",
    "        self.mid_res1 = ResBlock(in_channels=current_channels,\n",
    "                                 out_channels=current_channels)\n",
    "        \n",
    "        self.mid_attn = nn.Identity()#AttnBlock(in_channels=current_channels)\n",
    "        \n",
    "        self.mid_res2 = ResBlock(in_channels=current_channels,\n",
    "                                 out_channels=current_channels)\n",
    "        \n",
    "        self.norm_out = norm(current_channels)\n",
    "        self.pre_vq_conv = nn.Conv2d(in_channels=current_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv_in(x)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        h = self.mid_res1(h)\n",
    "        h = self.mid_attn(h)\n",
    "        h = self.mid_res2(h)\n",
    "        h = self.norm_out(h)\n",
    "        h = activation(h)\n",
    "        h = self.pre_vq_conv(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=config['embed_dim'],\n",
    "                 conv_in_channels=config['conv_in_channels'],\n",
    "                 channels_mult=config['channels_mult'],\n",
    "                 out_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=conv_in_channels * channels_mult[-1] ,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding='same')\n",
    "        current_channels = conv_in_channels * channels_mult[-1]\n",
    "        \n",
    "        self.mid_res1 = ResBlock(in_channels=current_channels,                                 \n",
    "                                 out_channels=current_channels)\n",
    "        \n",
    "        self.mid_attn = nn.Identity()#AttnBlock(in_channels=current_channels)\n",
    "        \n",
    "        self.mid_res2 = ResBlock(in_channels=current_channels,\n",
    "                                 out_channels=current_channels)\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        for i, m in enumerate(reversed((1,) + channels_mult[:-1])): \n",
    "            blk_in = current_channels\n",
    "            blk_out = conv_in_channels * m\n",
    "            if i != 0:\n",
    "                layers.append(ResBlock(in_channels=blk_in,\n",
    "                                       out_channels=blk_out)) ### \n",
    "                layers.append(UpSample(in_channels=blk_out,\n",
    "                                       out_channels=blk_out))\n",
    "                \n",
    "            else:\n",
    "                layers.append(ResBlock(in_channels=blk_in,\n",
    "                                       out_channels=blk_out))\n",
    "            current_channels = blk_out\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.norm_out = norm(current_channels)\n",
    "        self.conv_out = nn.Conv2d(in_channels=current_channels,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  stride=1,\n",
    "                                  padding='same')\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv_in(x)\n",
    "        h = self.mid_res1(h)\n",
    "        h = self.mid_attn(h)\n",
    "        h = self.mid_res2(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        h = self.norm_out(h)\n",
    "        h = activation(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class EmbeddingEMA(nn.Module):\n",
    "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        weight = torch.randn(num_tokens, codebook_dim)\n",
    "        self.weight = nn.Parameter(weight, requires_grad=False)\n",
    "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad=False)\n",
    "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad=False)\n",
    "        self.update = True\n",
    "\n",
    "    def forward(self, embed_id):\n",
    "        return F.embedding(embed_id, self.weight)\n",
    "\n",
    "    def cluster_size_ema_update(self, new_cluster_size):\n",
    "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
    "\n",
    "    def embed_avg_ema_update(self, new_embed_avg):\n",
    "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
    "\n",
    "    def weight_update(self, num_tokens):\n",
    "        n = self.cluster_size.sum()\n",
    "        smoothed_cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
    "        )\n",
    "        # normalize embedding average with smoothed cluster size\n",
    "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
    "        self.weight.data.copy_(embed_normalized)\n",
    "    \n",
    "    def reinit(self,probs):\n",
    "        n,l=self.weight.shape\n",
    "        for i in range(n):\n",
    "            if probs[i]==0:\n",
    "                init.uniform_(self.weight[i], -1.0, 1.0)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class CodeBook(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=config['embed_dim'], n_embed=config['n_embed'],\n",
    "                 beta=config['codebook_beta'], decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_embed = n_embed\n",
    "        self.embed = EmbeddingEMA(num_tokens=n_embed, codebook_dim=embed_dim,\n",
    "                                  decay=decay, eps=eps)\n",
    "        # weight[num_embeddings, embedding_dim]\n",
    "        init.uniform_(self.embed.weight, -1.0, 1.0)  # ?\n",
    "        self.beta = beta\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, z_e: torch.Tensor):\n",
    "        # b, c, h, w -> b, h, w, c\n",
    "        z_e = torch.permute(z_e, [0, 2, 3, 1]).contiguous()\n",
    "        z_flat = z_e.view(-1, self.embed_dim)\n",
    "\n",
    "        d = torch.cdist(z_flat, self.embed.weight, p=2)\n",
    "\n",
    "        closest_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embed(closest_indices).view(z_e.shape)\n",
    "        loss = torch.mean(self.beta * (z_e - z_q.detach()) ** 2)\n",
    "\n",
    "        encodings = F.one_hot(closest_indices, self.n_embed).type(z_e.dtype)\n",
    "        # EMA cluster size\n",
    "        encodings_sum = encodings.sum(axis=0).detach()\n",
    "\n",
    "        if self.training and self.embed.update:\n",
    "            with torch.no_grad():\n",
    "                self.embed.cluster_size_ema_update(encodings_sum)\n",
    "                # EMA embedding average\n",
    "                embed_sum = encodings.transpose(0, 1) @ z_flat\n",
    "                self.embed.embed_avg_ema_update(embed_sum)\n",
    "                # normalize embed_avg and update weight\n",
    "                self.embed.weight_update(self.n_embed)\n",
    "\n",
    "        z_q = z_e + (z_q - z_e).detach()\n",
    "        # b, h, w, c -> b, c, h, w\n",
    "        z_q = (torch.permute(z_q, [0, 3, 1, 2])).contiguous()\n",
    "\n",
    "        return {'z_q': z_q, 'loss': loss, 'encodings_sum': encodings_sum,\n",
    "               'encodings': closest_indices.detach()}\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3,\n",
    "                 conv_in_channels=d_config['conv_in_channels'],\n",
    "                 out_channels=1,\n",
    "                 channels_mult=d_config['channels_mult']): # (1,2,4)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=conv_in_channels * channels_mult[0],\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2,\n",
    "                                 padding=1)\n",
    "        current_channels = conv_in_channels * channels_mult[0]\n",
    "        self.conv_in_activation = nn.LeakyReLU(0.2,True)\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        for i, m in enumerate(channels_mult[1:]): \n",
    "            blk_in = current_channels\n",
    "            blk_out = conv_in_channels * m\n",
    "            layers.append(nn.Conv2d(in_channels=blk_in,\n",
    "                                    out_channels=blk_out,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1,\n",
    "                                    bias=True)) \n",
    "            layers.append(d_norm(blk_out))\n",
    "            layers.append(nn.LeakyReLU(0.2,True))\n",
    "            current_channels = blk_out   \n",
    "        \n",
    "        layers.append(nn.Conv2d(in_channels=current_channels,\n",
    "                                out_channels=current_channels,\n",
    "                                kernel_size=4,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True)) \n",
    "        layers.append(d_norm(current_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2,True))\n",
    "        \n",
    "        self.layers=layers\n",
    "\n",
    "        self.conv_out = nn.Conv2d(in_channels=current_channels,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=4,\n",
    "                                  stride=1,\n",
    "                                  padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv_in(x)\n",
    "        h = self.conv_in_activation(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        h = self.conv_out(h)\n",
    "        return F.sigmoid(h)\n",
    "    \n",
    "\n",
    "class VQGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=3, embed_dim=config['embed_dim'], use_disc=config['use_disc']):\n",
    "        super().__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.encoder = Encoder(in_channels=channels, out_channels=embed_dim)\n",
    "        self.code_book = CodeBook(embed_dim=embed_dim)\n",
    "        self.decoder = Decoder(in_channels=embed_dim, out_channels=channels)\n",
    "        if use_disc and phase==1:\n",
    "            self.discriminator = Discriminator(in_channels=channels, out_channels=1)\n",
    "        self.rec_loss_weight = config['rec_loss_weight']\n",
    "        self.gan_loss_weight = config['gan_loss_weight']\n",
    "        self.disc_loss_weight = 1\n",
    "        self.use_disc= use_disc and phase==1\n",
    "        if config['lpips_loss_weight']>0 and phase==1:\n",
    "            self.lpips_loss_weight = config['lpips_loss_weight']\n",
    "            self.lpips_fn = lpips.LPIPS(net=config['lpips_type'])\n",
    "            if config['cuda']:\n",
    "                self.lpips_fn.cuda()\n",
    "        else:\n",
    "            self.lpips_loss_weight = None\n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_gan_loss_weight(self,epoch,d_loss_val):\n",
    "        return\n",
    "        #if epoch<=3:\n",
    "         #   self.gan_loss_weight = 0.15\n",
    "        #elif 5<epoch<9:\n",
    "        #    self.gan_loss_weight = 0.06*(epoch-4)\n",
    "        #else:\n",
    "         #   self.gan_loss_weight = 0.3\n",
    "        #self.gan_loss_weight = self.gan_loss_weight * max(0.2,-d_loss_val)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def update_disc_loss_weight(self,epoch,d_loss_val):\n",
    "        return\n",
    "        #self.disc_loss_weight = 1 #max(0.5,(d_loss_val+1)/2)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        codebook_output = self.code_book(z_e)\n",
    "        return codebook_output['encodings']\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode(self, ind):\n",
    "        z_q = self.code_book.embed(ind).view([-1, config['latent_size'][0],\n",
    "                                              config['latent_size'][1],\n",
    "                                              self.embed_dim]).contiguous()\n",
    "        z_q = torch.permute(z_q, [0, 3, 1, 2]).contiguous()\n",
    "        recx = self.decoder(z_q)\n",
    "        return recx\n",
    "    \n",
    "    def calculate_adaptive_weight(self, nll_loss, g_loss):\n",
    "        last_layer = self.decoder.conv_out.weight\n",
    "        nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=False)[0]\n",
    "        g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=False)[0]\n",
    "        \n",
    "        d_weight = torch.norm(nll_grads) / (torch.norm(g_grads) + 1e-4)\n",
    "        d_weight = torch.clamp(d_weight, 0.0, 1e4).detach()\n",
    "        # d_weight = d_weight * self.discriminator_weight\n",
    "        return d_weight\n",
    "    \n",
    "    def reconstruct(self,x):\n",
    "        z_e = self.encoder(x)\n",
    "        codebook_output = self.code_book(z_e)\n",
    "        z_q, codebook_loss = codebook_output['z_q'], codebook_output['loss']\n",
    "        recx = self.decoder(z_q)\n",
    "        recx = torch.clip(recx,-1,1)\n",
    "\n",
    "        return {'recx':recx,\n",
    "                'codebook_loss':codebook_loss,\n",
    "                'encodings_sum':codebook_output['encodings_sum'],\n",
    "                'encodings':codebook_output['encodings'],\n",
    "                }\n",
    "    \n",
    "    def discriminate(self,fake_x,real_x=None,on_train=True):\n",
    "        if real_x is None:\n",
    "            self.discriminator.eval()\n",
    "            disc_out_fake = self.discriminator(fake_x)\n",
    "            if on_train:\n",
    "                self.discriminator.train()\n",
    "            disc_out_real=None\n",
    "        else:\n",
    "            disc_out_fake = self.discriminator(fake_x)\n",
    "            disc_out_real = self.discriminator(real_x)\n",
    "        return {'disc_out_fake':disc_out_fake,\n",
    "                'disc_out_real':disc_out_real}\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bce(proba,target=1):\n",
    "        if target==1:\n",
    "            return -torch.mean(torch.log(proba+1e-2))\n",
    "        elif target==0:\n",
    "            return -torch.mean(torch.log((1-proba)+1e-2))\n",
    "    \n",
    "    def calculate_g_loss(self,x,recx,codebook_loss,disc_out_fake=None):\n",
    "        rec_loss = torch.mean(torch.abs(recx - x)) #+ torch.abs(recx - x) * 0.05\n",
    "        lpips_loss = self.lpips_fn.forward(recx,x).mean()\n",
    "        if self.use_disc:\n",
    "            gan_loss = self.calculate_bce(disc_out_fake,target=1)\n",
    "            gan_acc = 1-torch.mean(disc_out_fake)\n",
    "        else:\n",
    "            gan_loss = 0\n",
    "            gan_acc = 0\n",
    "        tot_loss = rec_loss*self.rec_loss_weight+\\\n",
    "                   codebook_loss+\\\n",
    "                   lpips_loss*self.lpips_loss_weight+\\\n",
    "                   gan_loss*self.gan_loss_weight\n",
    "        return {'rec_loss':rec_loss,\n",
    "                'codebook_loss':codebook_loss,\n",
    "                'lpips_loss':lpips_loss,\n",
    "                'gan_loss':gan_loss,\n",
    "                'gan_accuracy':gan_acc,\n",
    "                'tot_loss':tot_loss\n",
    "               }\n",
    "    \n",
    "    def calculate_d_loss(self,disc_out_fake,disc_out_real):\n",
    "        disc_loss = self.calculate_bce(disc_out_fake,target=0)+\\\n",
    "                    self.calculate_bce(disc_out_real,target=1)\n",
    "        tot_loss = disc_loss * self.disc_loss_weight\n",
    "        disc_acc_r = torch.mean(disc_out_real)\n",
    "        disc_acc_f = 1-torch.mean(disc_out_fake)\n",
    "        return {'disc_loss':disc_loss,\n",
    "                'tot_loss':tot_loss,\n",
    "                'disc_accuracy_real':disc_acc_r,\n",
    "                'disc_accuracy_fake':disc_acc_f,\n",
    "               }\n",
    "\n",
    "class MaskGIT(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens=512, n_pos=143, \n",
    "                 embed_dim=128, nhead=8, hidden_dim=1024,\n",
    "                 n_layers=6,\n",
    "                 n_steps=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_steps=n_steps\n",
    "        self.n_pos=n_pos\n",
    "        self.embed_dim=embed_dim\n",
    "        self.n_tokens=n_tokens\n",
    "        \n",
    "        weight = torch.randn(n_tokens,embed_dim)\n",
    "        self.token_embed = nn.Parameter(weight, requires_grad=True)\n",
    "        init.trunc_normal_(self.token_embed, 0, 0.02)\n",
    "        weight = torch.randn(n_pos,embed_dim)\n",
    "        self.pos_embed = nn.Parameter(weight, requires_grad=True)\n",
    "        init.trunc_normal_(self.pos_embed, 0, 0.02)\n",
    "        \n",
    "        self.mask_embed=nn.Parameter(torch.zeros([embed_dim,]), requires_grad=True)\n",
    "        \n",
    "        self.embed_out=nn.ModuleList([nn.GELU(),\n",
    "                                      nn.LayerNorm(embed_dim)])\n",
    "        \n",
    "        self.encoder = nn.ModuleList([nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                                 nhead=nhead, \n",
    "                                                                 dim_feedforward=hidden_dim,\n",
    "                                                                 dropout=0.1, \n",
    "                                                                 activation='relu',\n",
    "                                                                 batch_first=True)\n",
    "                                      for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "        self.proj_out=nn.ModuleList([nn.Linear(embed_dim,embed_dim),\n",
    "                                     nn.GELU(),\n",
    "                                     nn.LayerNorm(embed_dim)])\n",
    "        self.bias = nn.Parameter(torch.zeros([n_pos,n_tokens]), requires_grad=True)\n",
    "        self.debugger=None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calculate_n_mask(self,x=None):\n",
    "        n = torch.cos(x*3.1415926535/2)*self.n_pos\n",
    "        n = torch.round(n).int()\n",
    "        return n.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_mask(self):\n",
    "        n=self.calculate_n_mask(x=torch.rand((1,)))\n",
    "        mask=torch.full((self.n_pos,),False,dtype=torch.bool)\n",
    "        r = torch.rand((self.n_pos,))\n",
    "        _, selected_positions = torch.topk(r, k=n, dim=-1) # (n_masked,)\n",
    "        mask[selected_positions]=True\n",
    "        return mask\n",
    "    \n",
    "    def embed(self,ind,mask=None): # ind/mask [batch_size,n_pos] or [n_pos,]\n",
    "        embedding = self.token_embed[ind] # [batch_size,n_pos,embed_dim] or [n_pos,embed_dim]\n",
    "        if mask is not None:\n",
    "            embedding[mask]=self.mask_embed # [n_masked,embed_dim] <- [embed_dim,]\n",
    "        embedding = embedding + self.pos_embed # [(batch_size,)n_pos,embed_dim]+[n_pos,embed_dim]\n",
    "        return embedding\n",
    "    \n",
    "    def train_val_step(self,x): # x[batch_size, n_pos]\n",
    "        masks=[]\n",
    "        for b in range(x.shape[0]):\n",
    "            masks.append(self.sample_mask())\n",
    "        mask=torch.vstack(masks)\n",
    "        embedding=self.embed(x,mask=mask)\n",
    "        logits=self.forward(masked_embedding=embedding)\n",
    "        return {'logits':logits,'mask':mask}\n",
    "    \n",
    "    def calculate_loss(self,x,logits,mask):\n",
    "        logits_=logits[mask].view(-1,self.n_tokens).contiguous()\n",
    "        x_=x[mask].view(-1).contiguous().long()\n",
    "        ce_loss=F.cross_entropy(logits_,\n",
    "                                target=x_,\n",
    "                                label_smoothing=0.1).mean()\n",
    "        log_proba = F.log_softmax(logits_.detach(),dim=-1)\n",
    "        raw_perplexity = torch.gather(log_proba,dim=1,index=x_.unsqueeze(-1)).mean()\n",
    "        return {\"tot_loss\":ce_loss,\"raw_perplexity\":raw_perplexity}\n",
    "        \n",
    "    def forward(self,masked_embedding):\n",
    "        h=masked_embedding\n",
    "        for layer in self.embed_out:\n",
    "            h=layer(h)\n",
    "        for layer in self.encoder:\n",
    "            h=layer(h)\n",
    "        for layer in self.proj_out:\n",
    "            h=layer(h)\n",
    "        logits = torch.matmul(h,self.token_embed.T) + self.bias\n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def unconditional_generate(self,temperature=(1,1),n_steps=None):\n",
    "        # assert batch_size == 1\n",
    "        if n_steps is None:\n",
    "            n_steps=self.n_steps\n",
    "        self.eval()\n",
    "        ind_ls=[]\n",
    "        current_ind = (torch.rand((self.n_pos,))*(self.n_tokens-1)).long() # [n_pos,]\n",
    "        n_masked=self.n_pos\n",
    "        mask = torch.full((self.n_pos,),True,dtype=torch.bool) # [n_pos,]\n",
    "        for t in range(1,n_steps):\n",
    "            embedding=self.embed(current_ind,mask=mask) # [n_pos,embed_dim]\n",
    "            logits=self.forward(masked_embedding=embedding.view(1,self.n_pos,self.embed_dim)).squeeze(0) # [n_pos,n_tokens]\n",
    "            masked_logits=logits.clone()[mask]/temperature[0] # [n_masked,n_tokens]\n",
    "            token_dis=torch.distributions.categorical.Categorical(logits=masked_logits)\n",
    "            token_sample=token_dis.sample() # [n_masked,]\n",
    "            token_confidence=torch.gather(token_dis.probs, # [n_masked,]\n",
    "                                          dim=-1,\n",
    "                                          index=token_sample.unsqueeze(-1)).squeeze(-1)\n",
    "            \n",
    "            sorted_confidence,_=torch.sort(token_confidence,\n",
    "                                           dim=-1,descending=True)\n",
    "            n=self.calculate_n_mask(x=torch.tensor(t/n_steps, \n",
    "                                                   dtype=torch.float32).view(1,))\n",
    "            dn=n_masked-n\n",
    "            n_masked=n\n",
    "            threshold_confidence=sorted_confidence[dn] # [1,]\n",
    "            confident_token_flag=token_confidence>threshold_confidence # [n_masked,]\n",
    "            current_ind[mask]=torch.where(confident_token_flag.cpu(),\n",
    "                                          token_sample.cpu(),\n",
    "                                          current_ind[mask])\n",
    "            mask[mask.clone()]=~confident_token_flag.cpu()\n",
    "\n",
    "            assert torch.abs(torch.sum(mask)-n_masked).cpu().item()<=1\n",
    "            ind_ls.append(current_ind.clone().squeeze(0))\n",
    "        embedding=self.embed(current_ind,mask=mask) # [n_pos,embed_dim]\n",
    "        logits=self.forward(masked_embedding=embedding.view(1,self.n_pos,self.embed_dim).contiguous()).squeeze(0) # [n_pos,n_tokens]\n",
    "        masked_logits=logits.clone()[mask] # [n_masked,n_token]\n",
    "        token_dis=torch.distributions.categorical.Categorical(logits=masked_logits)\n",
    "        token_sample=token_dis.sample() # [n_masked,]\n",
    "        current_ind[mask]=token_sample.cpu()\n",
    "        ind_ls.append(current_ind.clone().squeeze(0))\n",
    "        return ind_ls\n",
    "\n",
    "class EMALogger:\n",
    "    def __init__(self,decay=0.9):\n",
    "        self.decay=decay\n",
    "        self.val=0\n",
    "    def update(self,val):\n",
    "        self.val = self.decay*self.val + val*(1-self.decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.373814Z",
     "iopub.status.busy": "2024-04-12T06:37:15.373479Z",
     "iopub.status.idle": "2024-04-12T06:37:15.426224Z",
     "shell.execute_reply": "2024-04-12T06:37:15.425154Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.373789Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_phase1(epoch=0):\n",
    "    assert phase==1\n",
    "    torch.save(model.state_dict(),\n",
    "               os.path.join(outcome_root,f\"{config['ver']}/vq{epoch}.pth\"))\n",
    "    n_pos=config['latent_size'][0]*config['latent_size'][1]\n",
    "    model.eval()\n",
    "    acc_encodings = []\n",
    "    for _, batch_data in enumerate(train_dataloader):\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        encodings = model.encode(batch_data)\n",
    "        acc_encodings.append(np.reshape(encodings.cpu().numpy().astype('uint16'),\n",
    "                                        [-1,n_pos]))\n",
    "\n",
    "    for _, batch_data in enumerate(test_dataloader):\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        encodings = model.encode(batch_data)\n",
    "        acc_encodings.append(np.reshape(encodings.cpu().numpy().astype('uint16'),\n",
    "                                        [-1,n_pos]))\n",
    "    acc_encodings=np.vstack(acc_encodings)\n",
    "    np.save(os.path.join(outcome_root, f\"{config['ver']}/{epoch}_enc.npy\"),acc_encodings)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_phase2(epoch='maskgit'):\n",
    "    assert phase==2\n",
    "    maskgit.eval()\n",
    "    torch.save(maskgit.state_dict(),\n",
    "               os.path.join(outcome_root,f\"{config['ver']}/maskgit{epoch}.pth\"))\n",
    "\n",
    "    \n",
    "def print_num_params(model, name=\"\"):\n",
    "    with torch.no_grad():\n",
    "        num_params = 0\n",
    "        for param in model.parameters():\n",
    "            num_params += param.numel()\n",
    "        with open(config_path, 'a') as f:\n",
    "            f.write(f\"{name} parameters: {num_params}\\n\")\n",
    "        print(f\"{name} parameters: {num_params}\")\n",
    "        \n",
    "def train_step(model, epoch):\n",
    "    model.train()\n",
    "    n_batch = 0\n",
    "    acc_rec_loss = 0\n",
    "    acc_cb_loss = 0\n",
    "    acc_lpips_loss = 0\n",
    "    acc_gan_loss = 0\n",
    "    acc_gan_acc = 0\n",
    "    acc_encodings_sum = 0\n",
    "    acc_d_loss = 0\n",
    "    acc_d_acc_r = 0\n",
    "    acc_d_acc_f = 0\n",
    "    disc_skip = 0\n",
    "    for batch_ind, batch_data in enumerate(train_dataloader):\n",
    "        n_batch += 1\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        \n",
    "        d_optim.zero_grad()\n",
    "        rec_out = model.reconstruct(batch_data)\n",
    "        disc_out = model.discriminate(fake_x=rec_out['recx'].detach(),\n",
    "                                      real_x=batch_data,on_train=True)\n",
    "        d_loss = model.calculate_d_loss(disc_out['disc_out_fake'],\n",
    "                                        disc_out['disc_out_real'])\n",
    "        if d_loss['disc_accuracy_real']+d_loss['disc_accuracy_fake']<1.8:\n",
    "            d_loss['tot_loss'].backward()\n",
    "            d_optim.step()\n",
    "        else:\n",
    "            disc_skip+=1\n",
    "        \n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        disc_out = model.discriminate(fake_x=rec_out['recx'],on_train=True)\n",
    "        g_loss = model.calculate_g_loss(x=batch_data,\n",
    "                                        recx=rec_out['recx'],\n",
    "                                        codebook_loss=rec_out['codebook_loss'],\n",
    "                                        disc_out_fake=disc_out['disc_out_fake'])\n",
    "\n",
    "        g_loss['tot_loss'].backward()\n",
    "        g_optim.step()\n",
    "        \n",
    "       \n",
    "        d_loss_logger.update(d_loss['disc_loss'].detach().cpu().item())\n",
    "        \n",
    "        #model.update_gan_loss_weight(epoch=epoch,d_loss_val=d_loss_logger.val)\n",
    "        #model.update_disc_loss_weight(epoch=epoch,d_loss_val=d_loss_logger.val)\n",
    "        \n",
    "        acc_rec_loss += g_loss['rec_loss'].detach()\n",
    "        acc_cb_loss += g_loss['codebook_loss'].detach()\n",
    "        acc_lpips_loss += g_loss['lpips_loss'].detach()\n",
    "        acc_gan_loss += g_loss['gan_loss'].detach()\n",
    "        acc_gan_acc += g_loss['gan_accuracy'].detach()\n",
    "        acc_encodings_sum += rec_out['encodings_sum'].detach().cpu()\n",
    "        acc_d_loss += d_loss['disc_loss'].detach()\n",
    "        acc_d_acc_r += d_loss['disc_accuracy_real'].detach()\n",
    "        acc_d_acc_f += d_loss['disc_accuracy_fake'].detach()\n",
    "        \n",
    "    avg_probs = acc_encodings_sum / torch.sum(acc_encodings_sum)\n",
    "    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            vis_img(batch_data, rec_out['recx'], f\"train {epoch}\")\n",
    "    if epoch%config['save_every_n_epoch']==0:\n",
    "        save_phase1(epoch)\n",
    "    if epoch % 3 == 1 and epoch >= 3:\n",
    "        with torch.no_grad():\n",
    "            model.code_book.embed.reinit(avg_probs.cuda())\n",
    "    info = f'Train Epoch: {epoch}.\\n' +\\\n",
    "           f'rec_loss: {acc_rec_loss / n_batch:.4f}; ' +\\\n",
    "           f'codebook_loss: {acc_cb_loss / n_batch:.4f}; ' +\\\n",
    "           f'lpips_loss: {acc_lpips_loss / n_batch:.4f}; ' +\\\n",
    "           f'gan_loss: {acc_gan_loss / n_batch:.4f}; ' +\\\n",
    "           f'gan_accuray: {acc_gan_acc / n_batch:.4f}; ' +\\\n",
    "           f'perplexity: {perplexity:.4f}; ' +\\\n",
    "           f'disc_loss: {acc_d_loss / n_batch:.4f}; ' +\\\n",
    "           f'disc_accuracy: {acc_d_acc_r/n_batch:.4f}/{acc_d_acc_f/n_batch:.4f}; ' + \\\n",
    "           f'disc_skip: {disc_skip}\\n'\n",
    "    with open(config_path, 'a') as f:\n",
    "        f.write(info)\n",
    "\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def val_step(model, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_batch = 0\n",
    "        acc_rec_loss = 0\n",
    "        acc_cb_loss = 0\n",
    "        acc_lpips_loss = 0\n",
    "        acc_gan_loss = 0\n",
    "        acc_gan_acc = 0\n",
    "        acc_encodings_sum = 0\n",
    "        acc_d_loss = 0\n",
    "        acc_d_acc_r = 0\n",
    "        acc_d_acc_f = 0\n",
    "       \n",
    "        for batch_ind, batch_data in enumerate(test_dataloader):\n",
    "            n_batch += 1\n",
    "            if config['cuda']:\n",
    "                batch_data = batch_data.cuda()\n",
    "            rec_out = model.reconstruct(batch_data)\n",
    "            disc_out = model.discriminate(fake_x=rec_out['recx'],\n",
    "                                          real_x=batch_data,on_train=False)\n",
    "            d_loss = model.calculate_d_loss(disc_out['disc_out_fake'],\n",
    "                                            disc_out['disc_out_real'])\n",
    "            g_loss = model.calculate_g_loss(x=batch_data,\n",
    "                                            recx=rec_out['recx'],\n",
    "                                            codebook_loss=rec_out['codebook_loss'],\n",
    "                                            disc_out_fake=disc_out['disc_out_fake'])\n",
    "            acc_rec_loss += g_loss['rec_loss'].detach()\n",
    "            acc_cb_loss += g_loss['codebook_loss'].detach()\n",
    "            acc_lpips_loss += g_loss['lpips_loss'].detach()\n",
    "            acc_gan_loss += g_loss['gan_loss'].detach()\n",
    "            acc_gan_acc += g_loss['gan_accuracy'].detach()\n",
    "            acc_encodings_sum += rec_out['encodings_sum'].detach().cpu()\n",
    "            acc_d_loss += d_loss['disc_loss'].detach()\n",
    "            acc_d_acc_r += d_loss['disc_accuracy_real'].detach()\n",
    "            acc_d_acc_f += d_loss['disc_accuracy_fake'].detach()\n",
    "        avg_probs = acc_encodings_sum / torch.sum(acc_encodings_sum)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        if epoch % 1 == 0:\n",
    "            vis_img(batch_data, rec_out['recx'], f\"test {epoch}\")\n",
    "        info = f'Test Epoch: {epoch}.\\n' +\\\n",
    "               f'rec_loss: {acc_rec_loss / n_batch:.4f}; ' +\\\n",
    "               f'codebook_loss: {acc_cb_loss / n_batch:.4f}; ' +\\\n",
    "               f'lpips_loss: {acc_lpips_loss / n_batch:.4f}; ' +\\\n",
    "               f'gan_loss: {acc_gan_loss / n_batch:.4f}; ' +\\\n",
    "               f'gan_accuray: {acc_gan_acc / n_batch:.4f}; ' +\\\n",
    "               f'perplexity: {perplexity:.4f}; ' +\\\n",
    "               f'disc_loss: {acc_d_loss / n_batch:.4f}; ' +\\\n",
    "               f'disc_accuracy: {acc_d_acc_r/n_batch:.4f}/{acc_d_acc_f/n_batch:.4f}\\n'\n",
    "        with open(config_path, 'a') as f:\n",
    "            f.write(info)\n",
    "        if epoch==config['max_epochs']:\n",
    "            avg_probs=avg_probs.numpy()\n",
    "            avg_probs=np.sort(avg_probs)\n",
    "            plt.bar(np.arange(avg_probs.shape[0]),avg_probs)\n",
    "            plt.savefig(os.path.join(outcome_root, f\"{config['ver']}/Dis.png\"),dpi=80)\n",
    "            plt.clf()\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def train_step_vae(model, epoch):\n",
    "    model.train()\n",
    "    n_batch = 0\n",
    "    acc_rec_loss = 0\n",
    "    acc_cb_loss = 0\n",
    "    acc_lpips_loss = 0\n",
    "    acc_encodings_sum = 0\n",
    "    for batch_ind, batch_data in enumerate(train_dataloader):\n",
    "        n_batch += 1\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        \n",
    "        rec_out = model.reconstruct(batch_data)\n",
    "        g_optim.zero_grad()\n",
    "        g_loss = model.calculate_g_loss(x=batch_data,\n",
    "                                        recx=rec_out['recx'],\n",
    "                                        codebook_loss=rec_out['codebook_loss'],\n",
    "                                        disc_out_fake=None)\n",
    "\n",
    "        g_loss['tot_loss'].backward()\n",
    "        g_optim.step()\n",
    "        \n",
    "        acc_rec_loss += g_loss['rec_loss'].detach()\n",
    "        acc_cb_loss += g_loss['codebook_loss'].detach()\n",
    "        acc_lpips_loss += g_loss['lpips_loss'].detach()\n",
    "        acc_encodings_sum += rec_out['encodings_sum'].detach().cpu()\n",
    "        \n",
    "    avg_probs = acc_encodings_sum / torch.sum(acc_encodings_sum)\n",
    "    perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            vis_img(batch_data, rec_out['recx'], f\"train {epoch}\")\n",
    "    if epoch%config['save_every_n_epoch']==0:\n",
    "        save_phase1(epoch)\n",
    "    if epoch % 3 == 1 and epoch >= 3:\n",
    "        with torch.no_grad():\n",
    "            model.code_book.embed.reinit(avg_probs.cuda())\n",
    "    info = f'Train Epoch: {epoch}.\\n' +\\\n",
    "           f'rec_loss: {acc_rec_loss / n_batch:.4f}; ' +\\\n",
    "           f'codebook_loss: {acc_cb_loss / n_batch:.4f}; ' +\\\n",
    "           f'lpips_loss: {acc_lpips_loss / n_batch:.4f}; ' +\\\n",
    "           f'perplexity: {perplexity:.4f}\\n'\n",
    "           \n",
    "    with open(config_path, 'a') as f:\n",
    "        f.write(info)\n",
    "\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def val_step_vae(model, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_batch = 0\n",
    "        acc_rec_loss = 0\n",
    "        acc_cb_loss = 0\n",
    "        acc_lpips_loss = 0 \n",
    "        acc_encodings_sum = 0\n",
    "       \n",
    "        for batch_ind, batch_data in enumerate(test_dataloader):\n",
    "            n_batch += 1\n",
    "            if config['cuda']:\n",
    "                batch_data = batch_data.cuda()\n",
    "            rec_out = model.reconstruct(batch_data)\n",
    "            g_loss = model.calculate_g_loss(x=batch_data,\n",
    "                                            recx=rec_out['recx'],\n",
    "                                            codebook_loss=rec_out['codebook_loss'],\n",
    "                                            disc_out_fake=None)\n",
    "            acc_rec_loss += g_loss['rec_loss'].detach()\n",
    "            acc_cb_loss += g_loss['codebook_loss'].detach()\n",
    "            acc_lpips_loss += g_loss['lpips_loss'].detach()\n",
    "            acc_encodings_sum += rec_out['encodings_sum'].detach().cpu()\n",
    "        avg_probs = acc_encodings_sum / torch.sum(acc_encodings_sum)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        if epoch % 1 == 0:\n",
    "            vis_img(batch_data, rec_out['recx'], f\"test {epoch}\")\n",
    "        info = f'Test Epoch: {epoch}.\\n' +\\\n",
    "               f'rec_loss: {acc_rec_loss / n_batch:.4f}; ' +\\\n",
    "               f'codebook_loss: {acc_cb_loss / n_batch:.4f}; ' +\\\n",
    "               f'lpips_loss: {acc_lpips_loss / n_batch:.4f}; ' +\\\n",
    "               f'perplexity: {perplexity:.4f}\\n'\n",
    "        with open(config_path, 'a') as f:\n",
    "            f.write(info)\n",
    "        if epoch==config['max_epochs']:\n",
    "            avg_probs=avg_probs.numpy()\n",
    "            avg_probs=np.sort(avg_probs)\n",
    "            plt.bar(np.arange(avg_probs.shape[0]),avg_probs)\n",
    "            plt.savefig(os.path.join(outcome_root, f\"{config['ver']}/Dis.png\"),dpi=80)\n",
    "            plt.clf()\n",
    "    print(info)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vis_img(x, y, name):\n",
    "    h,w=config['image_size']\n",
    "    fp = os.path.join(outcome_root, f\"{config['ver']}/{name}.png\")\n",
    "    x = (cf2cl(x.detach().cpu()).numpy()[:8] + 1)/2\n",
    "    y = (cf2cl(y.detach().cpu()).numpy()[:8] + 1)/2\n",
    "    arr = np.zeros((4 * h, 4 * w, 3))\n",
    "    for i in [0, 1]:\n",
    "        for j in range(4):\n",
    "            arr[2 * i * h:(2 * i + 1) * h, \\\n",
    "            j * w:(j + 1) * w] = x[4 * i + j, :, :, :]\n",
    "    for i in [0, 1]:\n",
    "        for j in range(4):\n",
    "            arr[(2 * i + 1) * h:(2 * i + 2) * h, \\\n",
    "            j * w:(j + 1) * w] = y[4 * i + j, :, :, :]\n",
    "    arr = np.clip(arr * 255, 0, 255).astype(np.uint8)\n",
    "    cv2.imwrite(fp, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:37:15.428530Z",
     "iopub.status.busy": "2024-04-12T06:37:15.428214Z",
     "iopub.status.idle": "2024-04-12T06:37:15.450849Z",
     "shell.execute_reply": "2024-04-12T06:37:15.450011Z",
     "shell.execute_reply.started": "2024-04-12T06:37:15.428506Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step_maskgit(maskgit, epoch):\n",
    "    maskgit.train()\n",
    "    n_batch = 0\n",
    "    acc_ce_loss = 0\n",
    "    acc_raw_perplexity = 0\n",
    "    \n",
    "    for batch_ind, [batch_data] in enumerate(train_dataloader):\n",
    "        n_batch += 1\n",
    "        batch_data = batch_data.int()\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        \n",
    "        t_out = maskgit.train_val_step(batch_data)\n",
    "        t_optim.zero_grad()\n",
    "        t_loss = maskgit.calculate_loss(x=batch_data,\n",
    "                                        logits=t_out['logits'],\n",
    "                                        mask=t_out['mask'])\n",
    "\n",
    "        t_loss['tot_loss'].backward()\n",
    "        t_optim.step()\n",
    "        \n",
    "        acc_ce_loss += t_loss['tot_loss'].detach()\n",
    "        acc_raw_perplexity += t_loss['raw_perplexity'].detach()\n",
    "        \n",
    "    perplexity = torch.exp(-acc_raw_perplexity/n_batch)\n",
    "    if epoch%config['save_every_n_epoch']==0:\n",
    "        save_phase2(epoch)\n",
    "    info = f'Train Epoch: {epoch}.\\n' +\\\n",
    "           f'ce_loss: {acc_ce_loss / n_batch:.4f}; ' +\\\n",
    "           f'perplexity: {perplexity:.2f}\\n'  \n",
    "    with open(config_path, 'a') as f:\n",
    "        f.write(info)\n",
    "    print(info)\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def val_step_maskgit(maskgit, epoch):\n",
    "    maskgit.eval()\n",
    "    n_batch = 0\n",
    "    acc_ce_loss = 0\n",
    "    acc_raw_perplexity = 0\n",
    "    \n",
    "    for batch_ind, [batch_data] in enumerate(train_dataloader):\n",
    "        n_batch += 1\n",
    "        batch_data = batch_data.int()\n",
    "        if config['cuda']:\n",
    "            batch_data = batch_data.cuda()\n",
    "        \n",
    "        t_out = maskgit.train_val_step(batch_data)\n",
    "        t_loss = maskgit.calculate_loss(x=batch_data,\n",
    "                                        logits=t_out['logits'],\n",
    "                                        mask=t_out['mask'])      \n",
    "        acc_ce_loss += t_loss['tot_loss']\n",
    "        acc_raw_perplexity += t_loss['raw_perplexity']\n",
    "        \n",
    "    perplexity = torch.exp(-acc_raw_perplexity/n_batch)\n",
    "    info = f'Test Epoch: {epoch}.\\n' +\\\n",
    "           f'ce_loss: {acc_ce_loss / n_batch:.4f}; ' +\\\n",
    "           f'perplexity: {perplexity:.2f}\\n'  \n",
    "    vis_maskgit_unconditional_generate(batch_size=8,name=f\"m{epoch}\")\n",
    "    with open(config_path, 'a') as f:\n",
    "        f.write(info)\n",
    "    print(info)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vis_maskgit_unconditional_generate(batch_size=8,name='1',t=None):\n",
    "    if config['phase']!=3:\n",
    "        img_rows=[]\n",
    "        for _ in range(batch_size):\n",
    "            ind_ls=maskgit.unconditional_generate(temperature=(1,1)) # [n_step, 1, n_pos]\n",
    "            if config['cuda']:\n",
    "                ind=torch.vstack(ind_ls).cuda()\n",
    "            else:\n",
    "                ind=torch.vstack(ind_ls)\n",
    "            imgs = cf2cl(model.decode(ind).cpu()).numpy()\n",
    "            img_rows.append(np.hstack(imgs))\n",
    "        output=np.vstack(img_rows)\n",
    "        output=np.clip(output*127.5+127.5,0,255).astype(np.uint8)\n",
    "        fp = os.path.join(outcome_root, f\"{config['ver']}/{name}.png\")\n",
    "        cv2.imwrite(fp, output)\n",
    "    \n",
    "    steps_scheduler=lambda x: maskgit.n_steps-4+round(x/16*8)\n",
    "    temperature=t if t is not None else (1,1)\n",
    "    ind_ls=[]\n",
    "    for k in range(16):\n",
    "        ind_ls.append(maskgit.unconditional_generate(temperature=temperature,\n",
    "                                                     n_steps=steps_scheduler(k))[-1])\n",
    "    if config['cuda']:\n",
    "        ind=torch.vstack(ind_ls).cuda()\n",
    "    else:\n",
    "        ind=torch.vstack(ind_ls)\n",
    "    imgs = model.decode(ind)\n",
    "    vis_img(imgs[:8],imgs[8:],f\"{name}_\")\n",
    "    \n",
    "    temperature=t if t is not None else (2.5,1)\n",
    "    ind_ls=[]\n",
    "    for k in range(16):\n",
    "        ind_ls.append(maskgit.unconditional_generate(temperature=temperature,\n",
    "                                                     n_steps=steps_scheduler(k))[-1])\n",
    "    if config['cuda']:\n",
    "        ind=torch.vstack(ind_ls).cuda()\n",
    "    else:\n",
    "        ind=torch.vstack(ind_ls)\n",
    "    imgs = model.decode(ind)\n",
    "    vis_img(imgs[:8],imgs[8:],f\"{name}__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:38:40.665869Z",
     "iopub.status.busy": "2024-04-12T06:38:40.665442Z",
     "iopub.status.idle": "2024-04-12T06:39:06.262983Z",
     "shell.execute_reply": "2024-04-12T06:39:06.262091Z",
     "shell.execute_reply.started": "2024-04-12T06:38:40.665838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder parameters: 2629952\n",
      "CodeBook parameters: 131584\n",
      "Decoder parameters: 3015555\n",
      "MaskGIT parameters: 3875456\n"
     ]
    }
   ],
   "source": [
    "class CELEBAImageDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.target_size = config['image_size']\n",
    "        self.image_paths = image_paths\n",
    "        self.to_tensor=torchvision.transforms.ToTensor()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = cv2.imread(image_path)[10:,2:]\n",
    "        image = self.to_tensor((image/127.5-1).astype(np.float32))\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class AFHQImageDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.target_size = config['image_size']\n",
    "        self.intrp_method = cv2.INTER_LANCZOS4\n",
    "        self.image_paths = image_paths\n",
    "        self.to_tensor=torchvision.transforms.ToTensor()\n",
    "        self.flip=torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, self.target_size,interpolation=self.intrp_method)\n",
    "        image = self.to_tensor((image/127.5-1).astype(np.float32))\n",
    "        image = self.flip(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "if phase==1:\n",
    "    if 'afhq' in config['ver']:\n",
    "        split=round(config['dataset_size']/3*config['train_test_split'])\n",
    "        train_paths=[os.path.join(data_path_afhq,s,f\"{i:0>4d}.png\")\\\n",
    "                     for i in range(split)\\\n",
    "                     for s in ['cat','dog','wild']]\n",
    "        test_paths=[os.path.join(data_path_afhq,s,f\"{i:0>4d}.png\")\\\n",
    "                    for i in range(split,round(config['dataset_size']/3))\\\n",
    "                    for s in ['cat','dog','wild']]\n",
    "\n",
    "        train_data = AFHQImageDataset(train_paths)\n",
    "        test_data = AFHQImageDataset(test_paths)\n",
    "\n",
    "    elif 'celeba' in config['ver']:\n",
    "        split=round(config['dataset_size']*config['train_test_split'])\n",
    "        train_paths=[os.path.join(data_path_celeba,f\"{i:0>6d}.jpg\") for i in range(1,split+1)]\n",
    "        test_paths=[os.path.join(data_path_celeba,f\"{i:0>6d}.jpg\") for i in range(split+1,config['dataset_size']+1)]\n",
    "\n",
    "        train_data = CELEBAImageDataset(train_paths)\n",
    "        test_data = CELEBAImageDataset(test_paths)\n",
    "\n",
    "    print(f\"train data shape: {len(train_data)}\\ntest data shape: {len(test_data)}\")\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=config['batch_size'],\n",
    "                                  shuffle=True,num_workers=4,drop_last=True)\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=config['batch_size'],\n",
    "                                 shuffle=True,num_workers=4,drop_last=True)\n",
    "    \n",
    "elif phase==2:\n",
    "    ori_data = np.load(data_path_enc).astype('float32')\n",
    "\n",
    "    config['dataset_size']=min(config['dataset_size'],ori_data.shape[0])\n",
    "    split_ind = int(config['dataset_size'] * config['train_test_split'])\n",
    "    train_data, test_data = ori_data[:split_ind], ori_data[split_ind:config['dataset_size']]\n",
    "    print(f\"train data shape: {train_data.shape}\\ntest data shape: {test_data.shape}\")\n",
    "\n",
    "    train_data_tensor = torch.from_numpy(train_data)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=t_config['batch_size'],\n",
    "                                  shuffle=True,drop_last=True)\n",
    "\n",
    "    test_data_tensor = torch.from_numpy(test_data)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=t_config['batch_size'],\n",
    "                                 shuffle=True,drop_last=True)\n",
    "\n",
    "model = VQGAN(channels=3)\n",
    "device='cuda' if config['cuda'] else 'cpu'\n",
    "if config['load']:\n",
    "    model.load_state_dict(torch.load(config['load'],\n",
    "                                     map_location=torch.device(device)),\n",
    "                          strict=False)\n",
    "    print(\"vq_loaded\")\n",
    "model.eval()\n",
    "\n",
    "print_num_params(model.encoder,\"Encoder\")\n",
    "print_num_params(model.code_book,\"CodeBook\")\n",
    "print_num_params(model.decoder,\"Decoder\")\n",
    "if phase==1:\n",
    "    print_num_params(model.lpips_fn,\"LPIPS\")\n",
    "\n",
    "if config['use_disc']:\n",
    "    print_num_params(model.discriminator,\"Discriminator\")\n",
    "if config['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "if phase==1:\n",
    "    g_optim = torch.optim.Adam(list(model.encoder.parameters())+\\\n",
    "                               list(model.decoder.parameters()),\n",
    "                               lr=config['base_learning_rate'],\n",
    "                               betas=(0.5,0.9))\n",
    "    g_scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(g_optim,\n",
    "                                                           T_max=config['max_epochs'],\n",
    "                                                           eta_min=config['min_learning_rate'])\n",
    "if config['use_disc'] and phase==1:\n",
    "    d_optim = torch.optim.Adam(model.discriminator.parameters(),\n",
    "                               lr=d_config['base_learning_rate'],\n",
    "                               betas=(0.5,0.9))\n",
    "\n",
    "if phase==2 or phase==3:\n",
    "    maskgit=MaskGIT(n_tokens=t_config['n_tokens'], n_pos=t_config['n_pos'], \n",
    "                    embed_dim=t_config['embed_dim'], nhead=t_config['nhead'],\n",
    "                    hidden_dim=t_config['hidden_dim'],n_layers=t_config['n_layers'],\n",
    "                    n_steps=t_config['n_steps'])\n",
    "    device='cuda' if config['cuda'] else 'cpu'\n",
    "    if t_config['load']:\n",
    "        maskgit.load_state_dict(torch.load(t_config['load'],\n",
    "                                           map_location=torch.device(device)),\n",
    "                                strict=False)\n",
    "        print(\"maskgit_loaded\")\n",
    "    maskgit.eval()\n",
    "    print_num_params(maskgit,\"MaskGIT\")\n",
    "    if config['cuda']:\n",
    "        maskgit.cuda()\n",
    "    t_optim=torch.optim.Adam(maskgit.parameters(),\n",
    "                             lr=t_config['base_learning_rate'],\n",
    "                             betas=(0.9, 0.96))\n",
    "    t_scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(t_optim,\n",
    "                                                           T_max=config['max_epochs'],\n",
    "                                                           eta_min=t_config['min_learning_rate'])\n",
    "if phase==1 or phase==2:\n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "    d_loss_logger=EMALogger(decay=0.9)\n",
    "    for epoch in range(1, config['max_epochs'] + 1):\n",
    "        t0 = t1\n",
    "        if phase==1:\n",
    "            if config['use_disc']:\n",
    "                train_step(model, epoch)\n",
    "                val_step(model, epoch)\n",
    "            else:\n",
    "                train_step_vae(model, epoch)\n",
    "                val_step_vae(model, epoch)\n",
    "                g_scheduler.step()\n",
    "        elif phase==2:\n",
    "            train_step_maskgit(maskgit,epoch)\n",
    "            val_step_maskgit(maskgit,epoch)\n",
    "            t_scheduler.step()\n",
    "        t1 = time.time()\n",
    "        print(f\"time: {t1 - t0:.2f}\")\n",
    "\n",
    "    with open(config_path, 'a') as f:\n",
    "        f.write(f'time: {t1 - t0:.2f}\\n')\n",
    "\n",
    "    if config['save'] and config['max_epochs']%config['save_every_n_epoch']!=0:\n",
    "        if phase==1:\n",
    "            save_phase1(config['max_epochs'])\n",
    "        elif phase==2:\n",
    "            save_phase2(config['max_epochs'])\n",
    "elif phase==3:\n",
    "    for i in np.linspace(0.5,4.5,9):\n",
    "        vis_maskgit_unconditional_generate(batch_size=8,name=f\"{i:.1f}_01\",t=(i,1))\n",
    "        vis_maskgit_unconditional_generate(batch_size=8,name=f\"{i:.1f}_02\",t=(i,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:39:06.264925Z",
     "iopub.status.busy": "2024-04-12T06:39:06.264606Z",
     "iopub.status.idle": "2024-04-12T06:39:07.641177Z",
     "shell.execute_reply": "2024-04-12T06:39:07.640140Z",
     "shell.execute_reply.started": "2024-04-12T06:39:06.264900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "if phase==1:\n",
    "    X = model.code_book.embed.weight.detach().cpu().numpy()\n",
    "    pca = PCA(n_components=2)\n",
    "    X_2d = pca.fit_transform(X)\n",
    "    plt.scatter(X[:,0],X[:,1])\n",
    "    plt.savefig(os.path.join(outcome_root, f\"{config['ver']}/PCA.png\"),dpi=80)\n",
    "    plt.clf()\n",
    "elif phase==2 or phase==3:\n",
    "    def f(k):\n",
    "        if 0<=k<=3: # 0 3\n",
    "            return k\n",
    "        elif 4<=k<=7: # 5 11\n",
    "            return 2*k-3\n",
    "        elif 8<=k<=11:  # 14 23\n",
    "            return 3*k-10\n",
    "        else:\n",
    "            return 0\n",
    "    X = maskgit.pos_embed.detach().cpu().numpy()\n",
    "    pca = PCA(n_components=24)\n",
    "    X = pca.fit_transform(X)\n",
    "    fig, axs = plt.subplots(3, 4)\n",
    "    #plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    for i,ax in enumerate(axs.flat):\n",
    "        k=f(i)\n",
    "        ax.imshow(np.reshape(X[:,k],config['latent_size']))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(str(k+1))\n",
    "    plt.tight_layout(pad=0.5, h_pad=0.5, w_pad=0.5)\n",
    "    plt.savefig(os.path.join(outcome_root, f\"{config['ver']}/PosEmbed.png\"),dpi=80)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T06:39:07.642603Z",
     "iopub.status.busy": "2024-04-12T06:39:07.642272Z",
     "iopub.status.idle": "2024-04-12T06:39:09.946703Z",
     "shell.execute_reply": "2024-04-12T06:39:09.945511Z",
     "shell.execute_reply.started": "2024-04-12T06:39:07.642552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/afhq_maskgit_0412_v01_phase3.zip'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(f\"/kaggle/working/{config['ver']}\", \"zip\", f\"/kaggle/working/vqvae/{config['ver']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3596605,
     "sourceId": 6257844,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4774380,
     "sourceId": 8087747,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4780603,
     "sourceId": 8096552,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4781067,
     "sourceId": 8097179,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
